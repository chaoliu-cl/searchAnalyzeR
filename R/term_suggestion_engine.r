#' ML Term Suggestion Engine
#'
#' This module provides intelligent term suggestion capabilities using machine learning
#' techniques to identify synonymous search terms with better performance potential.

#' ML Term Suggestion Engine
#'
#' @description
#' A machine learning-based system for suggesting synonymous search terms that may
#' improve search performance. Uses semantic embeddings, biomedical ontologies,
#' and performance-based learning to recommend better search strategies.
#'
#' @details
#' The MLTermEngine employs multiple ML techniques:
#' \itemize{
#'   \item \strong{Semantic Embeddings}: Uses pre-trained biomedical word embeddings
#'   \item \strong{Performance Learning}: Learns from historical search performance data
#'   \item \strong{Ontology Integration}: Leverages biomedical ontologies (MeSH, UMLS)
#'   \item \strong{Context Analysis}: Analyzes term co-occurrence patterns
#'   \item \strong{Adaptive Ranking}: Uses ML models to rank term suggestions by predicted performance
#' }
#'
#' @section Methods:
#' \describe{
#'   \item{\code{new()}}{Initialize the term suggestion engine}
#'   \item{\code{suggest_terms(terms, context, performance_data)}}{Generate synonym suggestions}
#'   \item{\code{eval_suggestions(original_terms, suggested_terms, validation_corpus)}}{Evaluate suggestion quality}
#'   \item{\code{train_model(historical_data)}}{Train ML model on performance data}
#' }
#'
#' @examples
#' # Initialize the ML term engine
#' engine <- MLTermEngine$new()
#'
#' # Basic term suggestion
#' original_terms <- c("diabetes", "treatment", "clinical trial")
#' suggestions <- engine$suggest_terms(
#'   terms = original_terms,
#'   max_suggestions = 5,
#'   min_similarity = 0.7
#' )
#' print(suggestions)
#'
#' # Evaluate suggestions with validation corpus
#' corpus <- data.frame(
#'   id = paste0("art", 1:100),
#'   title = paste("Research on", sample(c("diabetes", "diabetic", "glycemic"), 100, replace = TRUE)),
#'   abstract = paste("Study about", sample(c("treatment", "therapy", "intervention"), 100, replace = TRUE)),
#'   stringsAsFactors = FALSE
#' )
#'
#' evaluation <- engine$eval_suggestions(
#'   original_terms = original_terms,
#'   suggested_terms = suggestions$terms,
#'   validation_corpus = corpus
#' )
#' print(evaluation)
#'
#' @export
MLTermEngine <- R6::R6Class(
  "MLTermEngine",
  public = list(
    #' @field embedding_model Trained embedding model for semantic similarity
    embedding_model = NULL,

    #' @field performance_model ML model for predicting term performance
    performance_model = NULL,

    #' @field ontology_cache Cached ontology terms for quick lookup
    ontology_cache = NULL,

    #' @field training_history Historical performance data for model training
    training_history = NULL,

    #' @description
    #' Initialize the Term Suggestion Engine with ML models and ontology data.
    #' @param load_pretrained Logical, whether to load pre-trained biomedical embeddings
    #' @param cache_ontologies Logical, whether to cache biomedical ontology data
    #' @return No return value, called for side effects
    initialize = function(load_pretrained = TRUE, cache_ontologies = TRUE) {
      private$check_dependencies()

      if (load_pretrained) {
        private$load_embedding_model()
      }

      if (cache_ontologies) {
        private$initialize_ontology_cache()
      }

      # Initialize empty training history
      self$training_history <- list()

      cat("TermSuggestionEngine initialized successfully!\n")
      invisible(self)
    },

    #' Suggest synonymous terms using ML techniques
    #' @param terms Character vector of original search terms
    #' @param context Optional context information (domain, corpus type)
    #' @param performance_data Optional historical performance data for terms
    #' @param max_suggestions Maximum number of suggestions per term
    #' @param min_similarity Minimum semantic similarity threshold (0-1)
    #' @param include_variants Logical, whether to include morphological variants
    #' @return List with suggested terms and their predicted performance scores
    suggest_terms = function(terms, context = NULL, performance_data = NULL,
                             max_suggestions = 5, min_similarity = 0.6,
                             include_variants = TRUE) {

      if (length(terms) == 0) {
        stop("At least one search term must be provided")
      }

      suggestions <- list()

      for (term in terms) {
        cat("Generating suggestions for term:", term, "\n")

        # Get suggestions from multiple sources
        embedding_suggestions <- private$get_embedding_suggestions(
          term, max_suggestions, min_similarity
        )

        ontology_suggestions <- private$get_ontology_suggestions(
          term, max_suggestions
        )

        variant_suggestions <- if (include_variants) {
          private$get_morphological_variants(term)
        } else {
          character(0)
        }

        # Combine and deduplicate suggestions
        all_suggestions <- unique(c(
          embedding_suggestions$terms,
          ontology_suggestions,
          variant_suggestions
        ))

        # Remove the original term
        all_suggestions <- setdiff(all_suggestions, tolower(term))

        # Score suggestions using performance prediction
        if (length(all_suggestions) > 0) {
          scored_suggestions <- private$score_suggestions(
            original_term = term,
            suggested_terms = all_suggestions,
            context = context,
            performance_data = performance_data
          )

          # Rank and filter only if we have suggestions
          if (nrow(scored_suggestions) > 0) {
            top_suggestions <- scored_suggestions[
              order(scored_suggestions$predicted_performance, decreasing = TRUE),
              , drop = FALSE
            ][1:min(max_suggestions, nrow(scored_suggestions)), , drop = FALSE]

            # Get embedding similarities for top suggestions
            embedding_sims <- numeric(0)
            if (length(embedding_suggestions$terms) > 0) {
              matching_indices <- match(top_suggestions$term, embedding_suggestions$terms)
              embedding_sims <- embedding_suggestions$similarities[matching_indices[!is.na(matching_indices)]]
            }

            suggestions[[term]] <- list(
              original_term = term,
              suggestions = top_suggestions,
              embedding_similarity = embedding_sims,
              prediction_confidence = mean(top_suggestions$confidence, na.rm = TRUE)
            )
          } else {
            # No valid suggestions after scoring
            suggestions[[term]] <- list(
              original_term = term,
              suggestions = data.frame(
                term = character(0),
                predicted_performance = numeric(0),
                confidence = numeric(0),
                stringsAsFactors = FALSE
              ),
              embedding_similarity = numeric(0),
              prediction_confidence = 0
            )
          }
        } else {
          # No suggestions found
          suggestions[[term]] <- list(
            original_term = term,
            suggestions = data.frame(
              term = character(0),
              predicted_performance = numeric(0),
              confidence = numeric(0),
              stringsAsFactors = FALSE
            ),
            embedding_similarity = numeric(0),
            prediction_confidence = 0
          )
        }
      }

      class(suggestions) <- "term_suggestions"
      return(suggestions)
    },

    #' Evaluate quality of term suggestions
    #' @param original_terms Character vector of original terms
    #' @param suggested_terms Character vector of suggested terms
    #' @param validation_corpus Data frame with corpus for validation (optional - will auto-generate if NULL)
    #' @param gold_standard Optional vector of known relevant article IDs
    #' @return Evaluation results comparing original vs suggested terms
    eval_suggestions = function(original_terms, suggested_terms,
                                validation_corpus = NULL, gold_standard = NULL) {

      # Auto-generate validation corpus if not provided
      if (is.null(validation_corpus)) {
        cat("No validation corpus provided. Auto-generating synthetic corpus...\n")
        validation_corpus <- private$auto_generate_corpus(
          terms = c(original_terms, suggested_terms),
          corpus_size = 200
        )
        cat("Generated corpus with", nrow(validation_corpus), "articles\n")
      }

      # Simulate searches with original and suggested terms
      original_results <- private$simulate_term_search(original_terms, validation_corpus)
      suggested_results <- private$simulate_term_search(suggested_terms, validation_corpus)

      evaluation <- list(
        original_terms = original_terms,
        suggested_terms = suggested_terms,
        validation_corpus_size = nrow(validation_corpus),
        retrieval_comparison = list(
          original_count = length(original_results),
          suggested_count = length(suggested_results),
          overlap = length(intersect(original_results, suggested_results)),
          unique_to_suggested = length(setdiff(suggested_results, original_results))
        )
      )

      # If gold standard is available, calculate precision/recall
      if (!is.null(gold_standard)) {
        original_metrics <- calc_precision_recall(original_results, gold_standard)
        suggested_metrics <- calc_precision_recall(suggested_results, gold_standard)

        evaluation$performance_comparison <- list(
          original = original_metrics,
          suggested = suggested_metrics,
          improvement = list(
            precision_delta = suggested_metrics$precision - original_metrics$precision,
            recall_delta = suggested_metrics$recall - original_metrics$recall,
            f1_delta = suggested_metrics$f1_score - original_metrics$f1_score
          )
        )
      } else {
        # Auto-generate gold standard based on term relevance
        cat("No gold standard provided. Auto-generating based on term relevance...\n")
        auto_gold <- private$auto_generate_gold_standard(validation_corpus, c(original_terms, suggested_terms))

        if (length(auto_gold) > 0) {
          original_metrics <- calc_precision_recall(original_results, auto_gold)
          suggested_metrics <- calc_precision_recall(suggested_results, auto_gold)

          evaluation$performance_comparison <- list(
            original = original_metrics,
            suggested = suggested_metrics,
            improvement = list(
              precision_delta = suggested_metrics$precision - original_metrics$precision,
              recall_delta = suggested_metrics$recall - original_metrics$recall,
              f1_delta = suggested_metrics$f1_score - original_metrics$f1_score
            )
          )
          evaluation$auto_generated_gold <- TRUE
          evaluation$gold_standard_size <- length(auto_gold)
        }
      }

      # Calculate semantic diversity
      evaluation$semantic_analysis <- private$analyze_semantic_diversity(
        original_terms, suggested_terms
      )

      class(evaluation) <- "suggestion_evaluation"
      return(evaluation)
    },

    #' Train performance prediction model from historical data
    #' @param historical_data Data frame with term performance history
    #' @param model_type Type of ML model ("random_forest", "svm", "neural_network")
    #' @param cross_validate Logical, whether to perform cross-validation
    #' @return Trained model performance metrics
    train_model = function(historical_data, model_type = "random_forest",
                           cross_validate = TRUE) {

      if (nrow(historical_data) < 10) {
        warning("Insufficient historical data for model training (need >= 10 observations)")
        return(NULL)
      }

      # Feature engineering
      features <- private$extract_term_features(historical_data)

      # Prepare training data
      training_data <- cbind(features, performance = historical_data$performance)

      # Train model based on type
      if (model_type == "random_forest") {
        if (requireNamespace("randomForest", quietly = TRUE)) {
          self$performance_model <- randomForest::randomForest(
            performance ~ .,
            data = training_data,
            ntree = 500,
            importance = TRUE
          )
        } else {
          warning("randomForest package not available, using linear model")
          self$performance_model <- lm(performance ~ ., data = training_data)
        }
      } else if (model_type == "svm") {
        if (requireNamespace("e1071", quietly = TRUE)) {
          self$performance_model <- e1071::svm(
            performance ~ .,
            data = training_data,
            kernel = "radial"
          )
        } else {
          warning("e1071 package not available, using linear model")
          self$performance_model <- lm(performance ~ ., data = training_data)
        }
      } else {
        # Default to linear model
        self$performance_model <- lm(performance ~ ., data = training_data)
      }

      # Cross-validation if requested
      if (cross_validate && requireNamespace("caret", quietly = TRUE)) {
        cv_results <- private$cross_validate_model(training_data, model_type)

        return(list(
          model = self$performance_model,
          cv_results = cv_results,
          feature_importance = private$get_feature_importance(),
          training_size = nrow(training_data)
        ))
      } else {
        return(list(
          model = self$performance_model,
          training_size = nrow(training_data)
        ))
      }
    },

    #' Get model-based term rankings
    #' @param terms Character vector of terms to rank
    #' @param context Optional context for prediction
    #' @return Data frame with terms and predicted performance scores
    rank_terms = function(terms, context = NULL) {
      if (is.null(self$performance_model)) {
        warning("No performance model available. Train model first with train_performance_model()")
        return(data.frame(term = terms, predicted_score = NA, rank = seq_along(terms)))
      }

      # Extract features for terms
      term_features <- private$extract_term_features(
        data.frame(term = terms, stringsAsFactors = FALSE)
      )

      # Predict performance
      predictions <- predict(self$performance_model, term_features)

      # Create ranking
      results <- data.frame(
        term = terms,
        predicted_score = predictions,
        rank = rank(-predictions, ties.method = "first"),
        stringsAsFactors = FALSE
      )

      return(results[order(results$rank), ])
    }
  ),

  private = list(
    check_dependencies = function() {
      optional_packages <- c("text", "word2vec", "quanteda", "randomForest", "e1071", "caret")
      available <- sapply(optional_packages, function(pkg) requireNamespace(pkg, quietly = TRUE))

      if (!any(available)) {
        warning("Advanced ML packages not available. Install 'text', 'word2vec', or 'quanteda' for better performance")
      }

      invisible(TRUE)
    },

    load_embedding_model = function() {
      # Try to load pre-trained biomedical embeddings
      if (requireNamespace("word2vec", quietly = TRUE)) {
        # In practice, you would load actual pre-trained biomedical embeddings
        # For demo, we'll create a simple embedding space

        # Create sample biomedical vocabulary
        biomedical_vocab <- c(
          "diabetes", "diabetic", "glycemic", "hyperglycemia", "insulin", "glucose",
          "treatment", "therapy", "intervention", "management", "care",
          "clinical", "trial", "study", "research", "investigation",
          "patient", "subject", "participant", "individual",
          "randomized", "controlled", "placebo", "double-blind"
        )

        # Simulate embedding space (in practice, load actual embeddings)
        self$embedding_model <- list(
          vocabulary = biomedical_vocab,
          dimensions = 300,
          similarity_matrix = private$create_similarity_matrix(biomedical_vocab)
        )

        cat("Biomedical embedding model loaded\n")
      } else {
        cat("Word embedding packages not available - using basic similarity\n")
      }
    },

    create_similarity_matrix = function(vocab) {
      # Create a mock similarity matrix for demonstration
      # In practice, this would be computed from actual embeddings
      n <- length(vocab)
      sim_matrix <- matrix(runif(n * n, 0.2, 0.9), nrow = n, ncol = n)
      diag(sim_matrix) <- 1.0

      # Make symmetric
      sim_matrix[lower.tri(sim_matrix)] <- t(sim_matrix)[lower.tri(sim_matrix)]

      rownames(sim_matrix) <- vocab
      colnames(sim_matrix) <- vocab

      return(sim_matrix)
    },

    initialize_ontology_cache = function() {
      # Initialize cache with common biomedical synonyms
      # In practice, this would connect to MeSH, UMLS, or other ontologies

      self$ontology_cache <- list(
        "diabetes" = c("diabetes mellitus", "diabetic", "hyperglycemia", "glycemic disorder"),
        "treatment" = c("therapy", "intervention", "management", "care", "therapeutic"),
        "clinical trial" = c("randomized trial", "controlled trial", "clinical study", "rct"),
        "patient" = c("subject", "participant", "individual", "case"),
        "drug" = c("medication", "pharmaceutical", "therapeutic agent", "medicine"),
        "cancer" = c("carcinoma", "tumor", "neoplasm", "malignancy", "oncology"),
        "heart disease" = c("cardiovascular disease", "cardiac", "coronary", "myocardial"),
        "infection" = c("infectious disease", "pathogen", "microbial", "sepsis")
      )

      cat("Ontology cache initialized\n")
    },

    get_embedding_suggestions = function(term, max_suggestions, min_similarity) {
      if (is.null(self$embedding_model)) {
        return(list(terms = character(0), similarities = numeric(0)))
      }

      vocab <- self$embedding_model$vocabulary
      sim_matrix <- self$embedding_model$similarity_matrix

      # Find term in vocabulary (case-insensitive)
      term_lower <- tolower(term)
      vocab_lower <- tolower(vocab)

      if (term_lower %in% vocab_lower) {
        term_idx <- which(vocab_lower == term_lower)[1]  # Take first match
        similarities <- sim_matrix[term_idx, ]

        # Filter by similarity threshold and remove self
        valid_indices <- which(similarities >= min_similarity & similarities < 1.0)

        if (length(valid_indices) > 0) {
          # Sort by similarity and take top suggestions
          sorted_indices <- valid_indices[order(similarities[valid_indices], decreasing = TRUE)]
          top_indices <- head(sorted_indices, max_suggestions)

          return(list(
            terms = vocab[top_indices],
            similarities = similarities[top_indices]
          ))
        }
      }

      return(list(terms = character(0), similarities = numeric(0)))
    },

    get_ontology_suggestions = function(term, max_suggestions) {
      if (is.null(self$ontology_cache)) {
        return(character(0))
      }

      # Direct lookup
      term_lower <- tolower(term)
      direct_match <- self$ontology_cache[[term_lower]]
      if (!is.null(direct_match)) {
        return(head(direct_match, max_suggestions))
      }

      # Fuzzy matching
      cache_keys <- names(self$ontology_cache)
      if (length(cache_keys) == 0) {
        return(character(0))
      }

      similarities <- sapply(cache_keys, function(key) {
        calc_text_sim(term_lower, key, method = "jaro_winkler")
      })

      if (length(similarities) > 0 && max(similarities) > 0.8) {
        best_match_key <- cache_keys[which.max(similarities)]
        return(head(self$ontology_cache[[best_match_key]], max_suggestions))
      }

      return(character(0))
    },

    get_morphological_variants = function(term) {
      # Generate morphological variants
      variants <- c()

      # Pluralization
      if (!endsWith(term, "s")) {
        variants <- c(variants, paste0(term, "s"))
      }

      # Common medical suffixes
      if (!endsWith(term, "ic")) {
        variants <- c(variants, paste0(term, "ic"))
      }

      if (!endsWith(term, "al")) {
        variants <- c(variants, paste0(term, "al"))
      }

      # Truncation for longer terms
      if (nchar(term) > 6) {
        variants <- c(variants, substr(term, 1, nchar(term) - 1))
        variants <- c(variants, substr(term, 1, nchar(term) - 2))
      }

      return(unique(variants))
    },

    score_suggestions = function(original_term, suggested_terms, context, performance_data) {
      if (length(suggested_terms) == 0) {
        return(data.frame(
          term = character(0),
          predicted_performance = numeric(0),
          confidence = numeric(0),
          stringsAsFactors = FALSE
        ))
      }

      scores <- data.frame(
        term = suggested_terms,
        predicted_performance = numeric(length(suggested_terms)),
        confidence = numeric(length(suggested_terms)),
        stringsAsFactors = FALSE
      )

      for (i in seq_along(suggested_terms)) {
        term <- suggested_terms[i]

        # Base score from semantic similarity
        base_score <- private$calculate_base_score(original_term, term)

        # Adjust based on performance model if available
        if (!is.null(self$performance_model)) {
          tryCatch({
            features <- private$extract_term_features(data.frame(term = term, stringsAsFactors = FALSE))
            model_score <- predict(self$performance_model, features)
            final_score <- 0.6 * base_score + 0.4 * as.numeric(model_score)
            confidence <- 0.8
          }, error = function(e) {
            # If model prediction fails, use base score
            final_score <<- base_score
            confidence <<- 0.5
          })
        } else {
          final_score <- base_score
          confidence <- 0.5
        }

        # Adjust based on historical performance if available
        if (!is.null(performance_data) && term %in% performance_data$term) {
          historical_perf <- performance_data$performance[performance_data$term == term]
          final_score <- 0.7 * final_score + 0.3 * mean(historical_perf, na.rm = TRUE)
          confidence <- min(confidence + 0.2, 1.0)
        }

        scores$predicted_performance[i] <- final_score
        scores$confidence[i] <- confidence
      }

      return(scores)
    },

    calculate_base_score = function(original_term, suggested_term) {
      # Calculate base performance score
      # This could be based on term frequency, domain specificity, etc.

      # Length penalty (very short terms may be too broad)
      length_score <- pmin(nchar(suggested_term) / 8, 1.0)

      # Semantic similarity bonus
      semantic_score <- calc_text_sim(original_term, suggested_term, method = "jaro_winkler")

      # Domain specificity (medical terms often contain specific patterns)
      domain_score <- if (grepl("(ology|itis|osis|therapy|treatment)$", suggested_term)) 0.8 else 0.6

      # Combine scores
      final_score <- 0.4 * length_score + 0.4 * semantic_score + 0.2 * domain_score

      return(final_score)
    },

    simulate_term_search = function(terms, corpus) {
      # Simulate search execution with given terms
      search_pattern <- paste(tolower(terms), collapse = "|")

      # Search in titles and abstracts
      title_matches <- grep(search_pattern, tolower(corpus$title))
      abstract_matches <- if ("abstract" %in% names(corpus)) {
        grep(search_pattern, tolower(corpus$abstract))
      } else {
        integer(0)
      }

      # Combine and deduplicate
      all_matches <- unique(c(title_matches, abstract_matches))

      return(corpus$id[all_matches])
    },

    analyze_semantic_diversity = function(original_terms, suggested_terms) {
      # Analyze semantic diversity between original and suggested terms
      all_terms <- c(original_terms, suggested_terms)

      # Calculate pairwise similarities
      similarities <- expand.grid(term1 = all_terms, term2 = all_terms, stringsAsFactors = FALSE)
      similarities$similarity <- mapply(function(t1, t2) {
        if (t1 == t2) return(1.0)
        calc_text_sim(t1, t2, method = "jaro_winkler")
      }, similarities$term1, similarities$term2)

      # Calculate diversity metrics
      avg_similarity <- mean(similarities$similarity[similarities$similarity < 1.0])
      max_similarity <- max(similarities$similarity[similarities$similarity < 1.0])
      min_similarity <- min(similarities$similarity[similarities$similarity < 1.0])

      return(list(
        average_similarity = avg_similarity,
        max_similarity = max_similarity,
        min_similarity = min_similarity,
        diversity_score = 1 - avg_similarity,  # Higher diversity = lower average similarity
        term_count = length(all_terms),
        unique_concepts = length(unique(tolower(all_terms)))
      ))
    },

    extract_term_features = function(data) {
      # Extract features for ML model training/prediction
      if (nrow(data) == 0) {
        return(data.frame(
          term_length = numeric(0),
          has_medical_suffix = numeric(0),
          word_count = numeric(0),
          has_hyphen = numeric(0),
          starts_with_upper = numeric(0),
          contains_digits = numeric(0)
        ))
      }

      features <- data.frame(
        term_length = nchar(data$term),
        has_medical_suffix = grepl("(ology|itis|osis|therapy|ic|al)$", data$term),
        word_count = sapply(strsplit(data$term, "\\s+"), length),
        has_hyphen = grepl("-", data$term),
        starts_with_upper = grepl("^[A-Z]", data$term),
        contains_digits = grepl("\\d", data$term),
        stringsAsFactors = FALSE
      )

      # Convert logical to numeric
      features$has_medical_suffix <- as.numeric(features$has_medical_suffix)
      features$has_hyphen <- as.numeric(features$has_hyphen)
      features$starts_with_upper <- as.numeric(features$starts_with_upper)
      features$contains_digits <- as.numeric(features$contains_digits)

      return(features)
    },

    cross_validate_model = function(training_data, model_type) {
      if (!requireNamespace("caret", quietly = TRUE)) {
        warning("caret package not available for cross-validation")
        return(NULL)
      }

      # Set up cross-validation
      ctrl <- caret::trainControl(method = "cv", number = 5)

      # Train with cross-validation
      if (model_type == "random_forest") {
        cv_model <- caret::train(
          performance ~ .,
          data = training_data,
          method = "rf",
          trControl = ctrl
        )
      } else {
        cv_model <- caret::train(
          performance ~ .,
          data = training_data,
          method = "lm",
          trControl = ctrl
        )
      }

      return(list(
        rmse = min(cv_model$results$RMSE),
        rsquared = max(cv_model$results$Rsquared, na.rm = TRUE),
        mae = min(cv_model$results$MAE)
      ))
    },

    get_feature_importance = function() {
      if (is.null(self$performance_model)) {
        return(NULL)
      }

      if (inherits(self$performance_model, "randomForest")) {
        importance_data <- randomForest::importance(self$performance_model)
        return(data.frame(
          feature = rownames(importance_data),
          importance = importance_data[, 1],
          stringsAsFactors = FALSE
        ))
      } else if (inherits(self$performance_model, "lm")) {
        coeffs <- summary(self$performance_model)$coefficients
        return(data.frame(
          feature = rownames(coeffs)[-1],  # Exclude intercept
          importance = abs(coeffs[-1, "t value"]),
          stringsAsFactors = FALSE
        ))
      }

      return(NULL)
    },

    # NEW: Auto-generate synthetic validation corpus
    auto_generate_corpus = function(terms, corpus_size = 200) {
      # Create diverse biomedical terms and concepts for realistic corpus
      base_medical_terms <- c(
        "diabetes", "diabetic", "glycemic", "hyperglycemia", "insulin", "glucose", "mellitus",
        "treatment", "therapy", "intervention", "management", "care", "therapeutic", "protocol",
        "clinical", "trial", "study", "research", "investigation", "experiment", "analysis",
        "patient", "subject", "participant", "individual", "cohort", "population",
        "randomized", "controlled", "placebo", "double-blind", "efficacy", "effectiveness",
        "cancer", "carcinoma", "tumor", "neoplasm", "malignancy", "oncology", "chemotherapy",
        "cardiovascular", "cardiac", "coronary", "myocardial", "hypertension", "cardiology",
        "infection", "infectious", "pathogen", "microbial", "sepsis", "antibiotic",
        "mental", "psychiatric", "depression", "anxiety", "cognitive", "behavioral",
        "surgery", "surgical", "operation", "procedure", "operative", "postoperative"
      )

      # Expand with provided terms
      all_terms <- unique(c(terms, base_medical_terms))

      # Generate synthetic articles
      articles <- data.frame(
        id = paste0("art_", sprintf("%04d", 1:corpus_size)),
        stringsAsFactors = FALSE
      )

      # Generate realistic titles
      title_templates <- c(
        "Effects of {term1} on {term2} in {population}",
        "A randomized trial of {term1} versus {term2}",
        "{term1} and {term2}: a systematic review",
        "Clinical efficacy of {term1} in treating {term2}",
        "Association between {term1} and {term2} outcomes",
        "Meta-analysis of {term1} interventions for {term2}",
        "Comparative effectiveness of {term1} and {term2}",
        "Long-term effects of {term1} on {term2} progression"
      )

      population_terms <- c("adults", "elderly patients", "children", "diabetic patients",
                            "cancer patients", "healthy volunteers", "hospitalized patients")

      articles$title <- sapply(1:corpus_size, function(i) {
        template <- sample(title_templates, 1)
        term1 <- sample(all_terms, 1)
        term2 <- sample(all_terms[all_terms != term1], 1)
        population <- sample(population_terms, 1)

        title <- gsub("\\{term1\\}", term1, template)
        title <- gsub("\\{term2\\}", term2, title)
        title <- gsub("\\{population\\}", population, title)
        return(title)
      })

      # Generate realistic abstracts
      abstract_templates <- c(
        "Background: {term1} is an important factor in {term2}. Methods: We conducted a study with {n} participants. Results: {term1} showed significant effects on {term2}. Conclusion: These findings suggest that {term1} may be beneficial for {term2} management.",
        "Objective: To evaluate the effectiveness of {term1} in {term2} treatment. Design: Randomized controlled trial with {n} subjects. Intervention: Participants received {term1} or placebo. Main outcome: {term2} improvement was measured. Results: {term1} demonstrated superior outcomes compared to control.",
        "Purpose: This study investigated the relationship between {term1} and {term2}. Methods: We analyzed data from {n} patients. Statistical analysis included regression modeling. Findings: Significant association was found between {term1} and {term2}. Clinical implications are discussed."
      )

      articles$abstract <- sapply(1:corpus_size, function(i) {
        template <- sample(abstract_templates, 1)
        term1 <- sample(all_terms, 1)
        term2 <- sample(all_terms[all_terms != term1], 1)
        n <- sample(c(50, 100, 150, 200, 500, 1000), 1)

        abstract <- gsub("\\{term1\\}", term1, template)
        abstract <- gsub("\\{term2\\}", term2, abstract)
        abstract <- gsub("\\{n\\}", n, abstract)
        return(abstract)
      })

      # Add metadata
      journals <- c("Journal of Medical Research", "Clinical Medicine", "Health Sciences Review",
                    "Biomedical Journal", "Medical Research Letters", "Clinical Investigation")

      articles$source <- sample(journals, corpus_size, replace = TRUE)
      articles$date <- Sys.Date() - sample(1:3650, corpus_size, replace = TRUE)  # Last 10 years

      return(articles)
    },

    # NEW: Auto-generate gold standard based on term relevance
    auto_generate_gold_standard = function(corpus, terms) {
      # Score each article based on term relevance
      relevance_scores <- numeric(nrow(corpus))

      for (i in 1:nrow(corpus)) {
        score <- 0
        text <- paste(tolower(corpus$title[i]), tolower(corpus$abstract[i]))

        # Score based on term presence and frequency
        for (term in terms) {
          term_count <- length(gregexpr(tolower(term), text, fixed = TRUE)[[1]])
          if (term_count > 0) {
            score <- score + term_count
          }
        }

        # Bonus for terms in title (more important)
        title_text <- tolower(corpus$title[i])
        for (term in terms) {
          if (grepl(tolower(term), title_text, fixed = TRUE)) {
            score <- score + 2
          }
        }

        relevance_scores[i] <- score
      }

      # Select top 20-30% as relevant (gold standard)
      threshold <- quantile(relevance_scores, 0.7, na.rm = TRUE)
      relevant_indices <- which(relevance_scores >= threshold & relevance_scores > 0)

      return(corpus$id[relevant_indices])
    }
  )
)

#' Auto-Generate Validation Corpus for Testing
#'
#' @param terms Character vector of terms to build corpus around
#' @param corpus_size Number of articles to generate (default: 200)
#' @param domain Domain focus ("medical", "general", "mixed")
#' @return Data frame with synthetic article corpus
#' @export
auto_generate_corpus <- function(terms = c("diabetes", "treatment"),
                                 corpus_size = 200,
                                 domain = "medical") {

  cat("Auto-generating validation corpus...\n")

  # Domain-specific term pools
  if (domain == "medical") {
    base_terms <- c(
      "diabetes", "diabetic", "glycemic", "hyperglycemia", "insulin", "glucose", "mellitus",
      "treatment", "therapy", "intervention", "management", "care", "therapeutic", "protocol",
      "clinical", "trial", "study", "research", "investigation", "experiment", "analysis",
      "patient", "subject", "participant", "individual", "cohort", "population",
      "randomized", "controlled", "placebo", "double-blind", "efficacy", "effectiveness",
      "cancer", "carcinoma", "tumor", "neoplasm", "malignancy", "oncology", "chemotherapy",
      "cardiovascular", "cardiac", "coronary", "myocardial", "hypertension", "cardiology",
      "infection", "infectious", "pathogen", "microbial", "sepsis", "antibiotic",
      "mental", "psychiatric", "depression", "anxiety", "cognitive", "behavioral",
      "surgery", "surgical", "operation", "procedure", "operative", "postoperative"
    )
  } else if (domain == "general") {
    base_terms <- c(
      "research", "study", "analysis", "investigation", "evaluation", "assessment",
      "method", "approach", "technique", "strategy", "framework", "model",
      "development", "implementation", "application", "innovation", "technology",
      "performance", "effectiveness", "efficiency", "quality", "improvement",
      "data", "information", "knowledge", "evidence", "findings", "results"
    )
  } else {  # mixed
    base_terms <- c(
      "research", "study", "analysis", "treatment", "therapy", "clinical",
      "patient", "data", "method", "results", "effectiveness", "intervention"
    )
  }

  # Combine with provided terms
  all_terms <- unique(c(terms, base_terms))

  # Generate articles
  articles <- data.frame(
    id = paste0("auto_", sprintf("%04d", 1:corpus_size)),
    stringsAsFactors = FALSE
  )

  # Title templates by domain
  if (domain == "medical") {
    title_templates <- c(
      "Effects of {term1} on {term2} in {population}",
      "A randomized trial of {term1} versus {term2}",
      "{term1} and {term2}: a systematic review",
      "Clinical efficacy of {term1} in treating {term2}",
      "Association between {term1} and {term2} outcomes",
      "Meta-analysis of {term1} interventions for {term2}",
      "Comparative effectiveness of {term1} and {term2}",
      "Long-term effects of {term1} on {term2} progression"
    )
    populations <- c("adults", "elderly patients", "children", "diabetic patients",
                     "cancer patients", "healthy volunteers", "hospitalized patients")
  } else {
    title_templates <- c(
      "Analysis of {term1} in {term2} contexts",
      "Investigating {term1} and {term2} relationships",
      "A comprehensive study of {term1} and {term2}",
      "Evaluation of {term1} methods for {term2}",
      "Research on {term1} applications in {term2}",
      "Comparative analysis of {term1} versus {term2}"
    )
    populations <- c("organizations", "systems", "environments", "contexts", "scenarios", "applications")
  }

  # Generate titles
  articles$title <- sapply(1:corpus_size, function(i) {
    template <- sample(title_templates, 1)
    term1 <- sample(all_terms, 1)
    term2 <- sample(all_terms[all_terms != term1], 1)
    population <- sample(populations, 1)

    title <- gsub("\\{term1\\}", term1, template)
    title <- gsub("\\{term2\\}", term2, title)
    title <- gsub("\\{population\\}", population, title)
    return(title)
  })

  # Generate abstracts
  if (domain == "medical") {
    abstract_templates <- c(
      "Background: {term1} is an important factor in {term2}. Methods: We conducted a study with {n} participants. Results: {term1} showed significant effects on {term2}. Conclusion: These findings suggest that {term1} may be beneficial for {term2} management.",
      "Objective: To evaluate the effectiveness of {term1} in {term2} treatment. Design: Randomized controlled trial with {n} subjects. Intervention: Participants received {term1} or placebo. Main outcome: {term2} improvement was measured. Results: {term1} demonstrated superior outcomes compared to control.",
      "Purpose: This study investigated the relationship between {term1} and {term2}. Methods: We analyzed data from {n} patients. Statistical analysis included regression modeling. Findings: Significant association was found between {term1} and {term2}. Clinical implications are discussed."
    )
  } else {
    abstract_templates <- c(
      "This research examines {term1} in the context of {term2}. We analyzed {n} cases using established methodologies. Our findings indicate significant relationships between {term1} and {term2}. Implications for future research are discussed.",
      "The study investigates {term1} applications for {term2}. Data from {n} sources were evaluated. Results demonstrate that {term1} provides effective solutions for {term2} challenges. Recommendations for implementation are provided.",
      "We present an analysis of {term1} and {term2} interactions. Using {n} observations, we developed models to understand these relationships. The research contributes to knowledge about {term1} effectiveness in {term2} scenarios."
    )
  }

  articles$abstract <- sapply(1:corpus_size, function(i) {
    template <- sample(abstract_templates, 1)
    term1 <- sample(all_terms, 1)
    term2 <- sample(all_terms[all_terms != term1], 1)
    n <- sample(c(50, 100, 150, 200, 500, 1000), 1)

    abstract <- gsub("\\{term1\\}", term1, template)
    abstract <- gsub("\\{term2\\}", term2, template)
    abstract <- gsub("\\{n\\}", n, abstract)
    return(abstract)
  })

  # Add metadata
  if (domain == "medical") {
    journals <- c("Journal of Medical Research", "Clinical Medicine", "Health Sciences Review",
                  "Biomedical Journal", "Medical Research Letters", "Clinical Investigation")
  } else {
    journals <- c("Research Journal", "Scientific Reports", "Journal of Applied Sciences",
                  "International Research", "Academic Studies", "Research Letters")
  }

  articles$source <- sample(journals, corpus_size, replace = TRUE)
  articles$date <- Sys.Date() - sample(1:3650, corpus_size, replace = TRUE)

  cat("Generated", corpus_size, "articles for", domain, "domain\n")
  cat("Articles contain variations of terms:", paste(head(all_terms, 5), collapse = ", "), "...\n")

  return(articles)
}

#' Auto-Generate Gold Standard for Evaluation
#'
#' @param corpus Data frame with article corpus
#' @param terms Character vector of terms to evaluate relevance against
#' @param relevance_threshold Percentile threshold for relevance (default: 0.7)
#' @return Vector of relevant article IDs
#' @export
auto_generate_gold <- function(corpus, terms, relevance_threshold = 0.7) {

  cat("Auto-generating gold standard based on term relevance...\n")

  # Score each article based on term relevance
  relevance_scores <- numeric(nrow(corpus))

  for (i in 1:nrow(corpus)) {
    score <- 0
    text <- paste(tolower(corpus$title[i]), tolower(corpus$abstract[i]))

    # Score based on term presence and frequency
    for (term in terms) {
      term_lower <- tolower(term)
      term_count <- length(gregexpr(term_lower, text, fixed = TRUE)[[1]])
      if (term_count > 0) {
        score <- score + term_count
      }
    }

    # Bonus for terms in title (more important)
    title_text <- tolower(corpus$title[i])
    for (term in terms) {
      if (grepl(tolower(term), title_text, fixed = TRUE)) {
        score <- score + 2
      }
    }

    relevance_scores[i] <- score
  }

  # Select top articles as relevant (gold standard)
  threshold <- quantile(relevance_scores, relevance_threshold, na.rm = TRUE)
  relevant_indices <- which(relevance_scores >= threshold & relevance_scores > 0)

  relevant_ids <- corpus$id[relevant_indices]

  cat("Generated gold standard with", length(relevant_ids), "relevant articles\n")
  cat("Relevance threshold:", threshold, "(", relevance_threshold*100, "th percentile)\n")

  return(relevant_ids)
}

#' Complete Automated Testing Workflow
#'
#' @param terms Character vector of terms to test
#' @param corpus_size Size of auto-generated corpus
#' @param domain Domain for corpus generation
#' @return Complete evaluation results
#' @export
auto_test_terms <- function(terms, corpus_size = 200, domain = "medical") {

  cat("=== Automated Term Testing Workflow ===\n\n")

  # Step 1: Generate validation corpus
  corpus <- auto_generate_corpus(terms, corpus_size, domain)

  # Step 2: Generate gold standard
  gold_standard <- auto_generate_gold(corpus, terms)

  # Step 3: Initialize ML engine and get suggestions
  engine <- MLTermEngine$new()
  suggestions <- engine$suggest_terms(terms, max_suggestions = 3)

  # Step 4: Extract suggested terms for evaluation
  suggested_terms <- unique(unlist(lapply(suggestions, function(x) {
    if (nrow(x$suggestions) > 0) x$suggestions$term else character(0)
  })))

  if (length(suggested_terms) == 0) {
    cat("No suggestions generated. Using original terms only.\n")
    suggested_terms <- terms
  }

  # Step 5: Evaluate suggestions
  evaluation <- engine$eval_suggestions(
    original_terms = terms,
    suggested_terms = suggested_terms,
    validation_corpus = corpus,
    gold_standard = gold_standard
  )

  # Step 6: Print summary
  cat("\n=== Evaluation Summary ===\n")
  cat("Original terms:", paste(terms, collapse = ", "), "\n")
  cat("Suggested terms:", paste(suggested_terms, collapse = ", "), "\n")
  cat("Corpus size:", nrow(corpus), "articles\n")
  cat("Gold standard size:", length(gold_standard), "relevant articles\n")

  if (!is.null(evaluation$performance_comparison)) {
    cat("\nPerformance Improvements:\n")
    cat("- Precision:", sprintf("%+.3f", evaluation$performance_comparison$improvement$precision_delta), "\n")
    cat("- Recall:", sprintf("%+.3f", evaluation$performance_comparison$improvement$recall_delta), "\n")
    cat("- F1 Score:", sprintf("%+.3f", evaluation$performance_comparison$improvement$f1_delta), "\n")
  }

  cat("\nRetrieval Comparison:\n")
  cat("- Original retrieval:", evaluation$retrieval_comparison$original_count, "articles\n")
  cat("- Suggested retrieval:", evaluation$retrieval_comparison$suggested_count, "articles\n")
  cat("- Unique to suggestions:", evaluation$retrieval_comparison$unique_to_suggested, "articles\n")

  return(list(
    corpus = corpus,
    gold_standard = gold_standard,
    suggestions = suggestions,
    evaluation = evaluation
  ))
}

#' Optimize Search Strategies
#'
#' @param search_strategies List of search strategy objects
#' @param validation_corpus Data frame with validation corpus
#' @param gold_standard Optional vector of relevant article IDs
#' @param optimization_method Method for optimization ("greedy", "genetic", "simulated_annealing")
#' @return Optimized search strategies with performance comparisons
#' @export
optimize_strategies <- function(search_strategies, validation_corpus,
                                gold_standard = NULL,
                                optimization_method = "greedy") {

  engine <- MLTermEngine$new()
  results <- list()

  for (strategy_name in names(search_strategies)) {
    cat("Optimizing strategy:", strategy_name, "\n")
    strategy <- search_strategies[[strategy_name]]

    # Generate suggestions for each term
    suggestions <- engine$suggest_terms(
      terms = strategy$terms,
      max_suggestions = 3,
      min_similarity = 0.6
    )

    # Create optimized strategy variants
    optimized_variants <- create_strategy_variants(
      original_strategy = strategy,
      suggestions = suggestions,
      method = optimization_method
    )

    # Evaluate each variant
    variant_performance <- lapply(optimized_variants, function(variant) {
      if (!is.null(gold_standard)) {
        # Simulate search with variant
        search_results <- sim_strategy_search(variant, validation_corpus)
        metrics <- calc_precision_recall(search_results, gold_standard)
        return(list(
          strategy = variant,
          performance = metrics,
          result_count = length(search_results)
        ))
      } else {
        # Just count results without gold standard
        search_results <- sim_strategy_search(variant, validation_corpus)
        return(list(
          strategy = variant,
          performance = NULL,
          result_count = length(search_results)
        ))
      }
    })

    # Select best variant
    if (!is.null(gold_standard)) {
      f1_scores <- sapply(variant_performance, function(x) x$performance$f1_score)
      best_idx <- which.max(f1_scores)
    } else {
      # Use result count as proxy (could be refined)
      result_counts <- sapply(variant_performance, function(x) x$result_count)
      best_idx <- which.max(result_counts)
    }

    results[[strategy_name]] <- list(
      original_strategy = strategy,
      optimized_strategy = variant_performance[[best_idx]]$strategy,
      performance_comparison = list(
        original = if (!is.null(gold_standard)) {
          orig_results <- sim_strategy_search(strategy, validation_corpus)
          calc_precision_recall(orig_results, gold_standard)
        } else NULL,
        optimized = variant_performance[[best_idx]]$performance
      ),
      all_variants = variant_performance,
      suggestions_used = suggestions
    )
  }

  class(results) <- "optimized_strategies"
  return(results)
}

#' Create Strategy Variants for Optimization
#'
#' @param original_strategy Original search strategy
#' @param suggestions Term suggestions from ML engine
#' @param method Optimization method
#' @return List of strategy variants
create_strategy_variants <- function(original_strategy, suggestions, method = "greedy") {
  variants <- list()

  if (method == "greedy") {
    # Greedy approach: try replacing each term with its best suggestion
    for (i in seq_along(original_strategy$terms)) {
      term <- original_strategy$terms[i]

      if (term %in% names(suggestions) &&
          nrow(suggestions[[term]]$suggestions) > 0) {

        # Create variant with best suggestion for this term
        variant_terms <- original_strategy$terms
        variant_terms[i] <- suggestions[[term]]$suggestions$term[1]

        variant <- original_strategy
        variant$terms <- variant_terms
        variant$optimization_note <- paste("Replaced", term, "with", variant_terms[i])

        variants[[paste0("variant_", i)]] <- variant
      }
    }

    # Create variant with all best suggestions
    all_best_terms <- sapply(original_strategy$terms, function(term) {
      if (term %in% names(suggestions) &&
          nrow(suggestions[[term]]$suggestions) > 0) {
        return(suggestions[[term]]$suggestions$term[1])
      } else {
        return(term)
      }
    })

    if (!identical(all_best_terms, original_strategy$terms)) {
      all_best_variant <- original_strategy
      all_best_variant$terms <- all_best_terms
      all_best_variant$optimization_note <- "All terms replaced with best suggestions"
      variants[["all_best"]] <- all_best_variant
    }

  } else if (method == "genetic") {
    # Simplified genetic algorithm approach
    variants <- genetic_optimize(original_strategy, suggestions)

  } else if (method == "simulated_annealing") {
    # Simplified simulated annealing approach
    variants <- sa_optimize(original_strategy, suggestions)
  }

  return(variants)
}

#' Simulate Strategy Search on Corpus
#'
#' @param strategy Search strategy object
#' @param corpus Validation corpus
#' @return Vector of retrieved article IDs
sim_strategy_search <- function(strategy, corpus) {
  # Simulate search execution with strategy terms
  search_pattern <- paste(tolower(strategy$terms), collapse = "|")

  # Search in titles and abstracts
  searchable_text <- paste(tolower(corpus$title),
                           if ("abstract" %in% names(corpus)) tolower(corpus$abstract) else "",
                           sep = " ")

  matches <- grep(search_pattern, searchable_text)

  return(corpus$id[matches])
}

#' Genetic Algorithm Optimization (Simplified)
#'
#' @param original_strategy Original strategy
#' @param suggestions Term suggestions
#' @return List of optimized variants
genetic_optimize <- function(original_strategy, suggestions) {
  # Simplified genetic algorithm implementation
  population_size <- 6
  variants <- list()

  # Create initial population
  for (i in 1:population_size) {
    variant_terms <- original_strategy$terms

    # Randomly replace some terms with suggestions
    for (j in seq_along(variant_terms)) {
      term <- variant_terms[j]
      if (term %in% names(suggestions) &&
          nrow(suggestions[[term]]$suggestions) > 0 &&
          runif(1) < 0.5) {  # 50% chance to replace

        # Choose random suggestion (not always the best)
        suggestion_idx <- sample(nrow(suggestions[[term]]$suggestions), 1)
        variant_terms[j] <- suggestions[[term]]$suggestions$term[suggestion_idx]
      }
    }

    variant <- original_strategy
    variant$terms <- variant_terms
    variant$optimization_note <- paste("Genetic variant", i)
    variants[[paste0("genetic_", i)]] <- variant
  }

  return(variants)
}

#' Simulated Annealing Optimization (Simplified)
#'
#' @param original_strategy Original strategy
#' @param suggestions Term suggestions
#' @return List of optimized variants
sa_optimize <- function(original_strategy, suggestions) {
  # Simplified simulated annealing implementation
  variants <- list()
  current_strategy <- original_strategy

  # Temperature schedule
  temperatures <- c(1.0, 0.7, 0.4, 0.1)

  for (temp_idx in seq_along(temperatures)) {
    temp <- temperatures[temp_idx]

    # Try modifications at this temperature
    variant_terms <- current_strategy$terms

    # Select term to modify based on temperature
    term_idx <- sample(length(variant_terms), 1)
    term <- variant_terms[term_idx]

    if (term %in% names(suggestions) &&
        nrow(suggestions[[term]]$suggestions) > 0) {

      # Choose suggestion based on temperature (higher temp = more exploration)
      if (temp > 0.5) {
        # High temp: random selection
        suggestion_idx <- sample(nrow(suggestions[[term]]$suggestions), 1)
      } else {
        # Low temp: favor better suggestions
        suggestion_idx <- 1  # Best suggestion
      }

      variant_terms[term_idx] <- suggestions[[term]]$suggestions$term[suggestion_idx]

      variant <- original_strategy
      variant$terms <- variant_terms
      variant$optimization_note <- paste("SA variant at temp", round(temp, 2))
      variants[[paste0("sa_", temp_idx)]] <- variant

      current_strategy <- variant
    }
  }

  return(variants)
}

#' Advanced Term Clustering for Strategy Development
#'
#' @param terms Character vector of terms to cluster
#' @param n_clusters Number of clusters to create
#' @param method Clustering method ("kmeans", "hierarchical")
#' @return List with cluster assignments and recommendations
#' @export
cluster_terms = function(terms, n_clusters = 3, method = "kmeans") {

  if (length(terms) < n_clusters) {
    warning("Number of terms is less than number of clusters requested")
    n_clusters <- length(terms)
  }

  # Create feature matrix for terms
  feature_matrix <- matrix(0, nrow = length(terms), ncol = length(terms))
  rownames(feature_matrix) <- terms
  colnames(feature_matrix) <- terms

  # Calculate pairwise similarities
  for (i in 1:length(terms)) {
    for (j in 1:length(terms)) {
      if (i != j) {
        feature_matrix[i, j] <- calc_text_sim(terms[i], terms[j], method = "jaro_winkler")
      } else {
        feature_matrix[i, j] <- 1.0
      }
    }
  }

  # Perform clustering
  if (method == "kmeans") {
    if (requireNamespace("stats", quietly = TRUE)) {
      distance_matrix <- 1 - feature_matrix
      clustering <- stats::kmeans(distance_matrix, centers = n_clusters, nstart = 20)
      cluster_assignments <- clustering$cluster
    } else {
      # Fallback to simple grouping
      cluster_assignments <- rep(1:n_clusters, length.out = length(terms))
    }
  } else if (method == "hierarchical") {
    distance_matrix <- as.dist(1 - feature_matrix)
    hc <- stats::hclust(distance_matrix)
    cluster_assignments <- stats::cutree(hc, k = n_clusters)
  }

  # Organize results by cluster
  clusters <- list()
  for (i in 1:n_clusters) {
    cluster_terms <- terms[cluster_assignments == i]

    # Get representative term (most central in cluster)
    if (length(cluster_terms) > 1) {
      cluster_similarities <- feature_matrix[cluster_terms, cluster_terms]
      centrality_scores <- rowMeans(cluster_similarities)
      representative <- names(which.max(centrality_scores))
    } else {
      representative <- cluster_terms[1]
    }

    clusters[[paste0("cluster_", i)]] <- list(
      terms = cluster_terms,
      representative = representative,
      size = length(cluster_terms),
      avg_internal_similarity = if (length(cluster_terms) > 1) {
        mean(feature_matrix[cluster_terms, cluster_terms][upper.tri(feature_matrix[cluster_terms, cluster_terms])])
      } else {
        1.0
      }
    )
  }

  return(list(
    clusters = clusters,
    cluster_assignments = cluster_assignments,
    n_clusters = n_clusters,
    method = method,
    silhouette_score = calc_silhouette(feature_matrix, cluster_assignments)
  ))
}

#' Calculate Silhouette Score for Clustering Quality
#'
#' @param similarity_matrix Similarity matrix between terms
#' @param cluster_assignments Cluster assignment vector
#' @return Average silhouette score
calc_silhouette <- function(similarity_matrix, cluster_assignments) {
  n <- length(cluster_assignments)
  silhouette_scores <- numeric(n)

  for (i in 1:n) {
    # Calculate average similarity within cluster
    same_cluster <- which(cluster_assignments == cluster_assignments[i])
    same_cluster <- same_cluster[same_cluster != i]

    if (length(same_cluster) > 0) {
      a_i <- mean(similarity_matrix[i, same_cluster])
    } else {
      a_i <- 0
    }

    # Calculate average similarity to nearest cluster
    other_clusters <- unique(cluster_assignments[cluster_assignments != cluster_assignments[i]])

    if (length(other_clusters) > 0) {
      b_scores <- sapply(other_clusters, function(cluster) {
        other_cluster_members <- which(cluster_assignments == cluster)
        mean(similarity_matrix[i, other_cluster_members])
      })
      b_i <- max(b_scores)
    } else {
      b_i <- 0
    }

    # Calculate silhouette score
    silhouette_scores[i] <- (b_i - a_i) / max(a_i, b_i)
  }

  return(mean(silhouette_scores))
}

#' Integration Methods for SearchAnalyzer Class
#' These functions extend the existing SearchAnalyzer with ML-based term suggestions

#' Add ML Term Suggestion Capability to SearchAnalyzer (Deprecated)
#'
#' @param analyzer SearchAnalyzer object
#' @param ml_engine Optional pre-initialized MLTermEngine
#' @return Modified SearchAnalyzer with term suggestion capabilities
#' @export
add_ml_terms <- function(analyzer, ml_engine = NULL) {
  warning("add_ml_terms() is deprecated due to R6 environment locking issues.\n",
          "Please use create_ml_analyzer() instead for ML-enhanced functionality.\n",
          "Example: analyzer <- create_ml_analyzer(search_results, gold_standard, search_strategy)")

  # Return the original analyzer unchanged
  return(analyzer)
}

#' Alternative ML-Enhanced SearchAnalyzer Factory
#'
#' @param search_results Data frame with search results
#' @param gold_standard Vector of known relevant article IDs
#' @param search_strategy List containing search parameters
#' @param ml_engine Optional pre-initialized MLTermEngine
#' @return SearchAnalyzer with built-in ML capabilities
#' @export
create_ml_analyzer <- function(search_results, gold_standard = NULL,
                               search_strategy = NULL, ml_engine = NULL) {

  # Create base analyzer
  base_analyzer <- SearchAnalyzer$new(search_results, gold_standard, search_strategy)

  # Initialize ML engine if not provided
  if (is.null(ml_engine)) {
    ml_engine <- MLTermEngine$new()
  }

  # Create a proper R6-like class for ML analyzer
  MLSearchAnalyzer <- R6::R6Class(
    "MLSearchAnalyzer",
    public = list(
      search_results = NULL,
      gold_standard = NULL,
      metadata = NULL,
      ml_engine = NULL,
      base_analyzer = NULL,

      initialize = function(base_analyzer, ml_engine) {
        self$search_results <- base_analyzer$search_results
        self$gold_standard <- base_analyzer$gold_standard
        self$metadata <- base_analyzer$metadata
        self$ml_engine <- ml_engine
        self$base_analyzer <- base_analyzer
      },

      calculate_metrics = function() {
        self$base_analyzer$calculate_metrics()
      },

      visualize_performance = function(type = "overview") {
        self$base_analyzer$visualize_performance(type)
      },

      suggest_terms = function(max_suggestions = 5, min_similarity = 0.6) {
        if (is.null(self$metadata$search_terms)) {
          stop("No search terms available in analyzer metadata")
        }

        suggestions <- self$ml_engine$suggest_terms(
          terms = self$metadata$search_terms,
          max_suggestions = max_suggestions,
          min_similarity = min_similarity,
          performance_data = extract_term_perf(self$search_results, self$metadata$search_terms)
        )

        return(suggestions)
      },

      eval_terms = function(suggested_terms, validation_corpus = NULL) {
        if (is.null(validation_corpus)) {
          validation_corpus <- self$search_results
        }

        evaluation <- self$ml_engine$eval_suggestions(
          original_terms = self$metadata$search_terms,
          suggested_terms = suggested_terms,
          validation_corpus = validation_corpus,
          gold_standard = self$gold_standard
        )

        return(evaluation)
      },

      has_ml_terms = function() {
        return(TRUE)
      },

      get_ml_engine = function() {
        return(self$ml_engine)
      },

      optimize_strategy = function(max_suggestions = 3, iterations = 2) {
        cat(" Starting ML-based strategy optimization...\n")

        current_terms <- self$metadata$search_terms
        optimization_history <- list()

        for (i in 1:iterations) {
          cat("Iteration", i, "of", iterations, "\n")

          # Generate suggestions
          suggestions <- self$suggest_terms(
            max_suggestions = max_suggestions,
            min_similarity = 0.7
          )

          # Extract best suggestions
          new_terms <- current_terms
          for (term_name in names(suggestions)) {
            if (nrow(suggestions[[term_name]]$suggestions) > 0) {
              best_suggestion <- suggestions[[term_name]]$suggestions$term[1]
              new_terms <- c(new_terms, best_suggestion)
              cat("  Added '", best_suggestion, "' for '", term_name, "'\n", sep = "")
            }
          }

          new_terms <- unique(new_terms)

          # Evaluate improvement
          if (!is.null(self$gold_standard)) {
            original_results <- sim_strategy_search(
              list(terms = current_terms),
              self$search_results
            )
            new_results <- sim_strategy_search(
              list(terms = new_terms),
              self$search_results
            )

            original_metrics <- calc_precision_recall(original_results, self$gold_standard)
            new_metrics <- calc_precision_recall(new_results, self$gold_standard)

            improvement <- new_metrics$f1_score - original_metrics$f1_score
            cat("  F1 improvement:", sprintf("%+.3f", improvement), "\n")

            optimization_history[[i]] <- list(
              iteration = i,
              original_f1 = original_metrics$f1_score,
              new_f1 = new_metrics$f1_score,
              improvement = improvement,
              terms_added = setdiff(new_terms, current_terms)
            )

            if (improvement > 0) {
              current_terms <- new_terms
              self$metadata$search_terms <- current_terms
            }
          } else {
            current_terms <- new_terms
            self$metadata$search_terms <- current_terms
          }
        }

        cat(" Optimization complete!\n")
        return(list(
          optimized_terms = current_terms,
          optimization_history = optimization_history
        ))
      }
    )
  )

  # Create and return the enhanced analyzer
  enhanced_analyzer <- MLSearchAnalyzer$new(base_analyzer, ml_engine)
  return(enhanced_analyzer)
}

#' Simple ML Integration Wrapper (Alternative approach)
#'
#' @param search_results Data frame with search results
#' @param gold_standard Vector of known relevant article IDs
#' @param search_strategy List containing search parameters
#' @return Simple wrapper with ML capabilities
#' @export
create_simple_ml_wrapper <- function(search_results, gold_standard = NULL, search_strategy = NULL) {

  # Create base components
  base_analyzer <- SearchAnalyzer$new(search_results, gold_standard, search_strategy)
  ml_engine <- MLTermEngine$new()

  # Create simple wrapper using closure
  wrapper <- function() {
    # Store components in closure
    analyzer_ref <- base_analyzer
    ml_ref <- ml_engine

    # Return methods object
    methods <- list()

    # Copy base analyzer data
    methods$search_results <- analyzer_ref$search_results
    methods$gold_standard <- analyzer_ref$gold_standard
    methods$metadata <- analyzer_ref$metadata

    # Base analyzer methods
    methods$calculate_metrics <- function() {
      analyzer_ref$calculate_metrics()
    }

    methods$visualize_performance <- function(type = "overview") {
      analyzer_ref$visualize_performance(type)
    }

    # ML methods
    methods$suggest_terms <- function(max_suggestions = 5, min_similarity = 0.6) {
      if (is.null(methods$metadata$search_terms)) {
        stop("No search terms available in analyzer metadata")
      }

      suggestions <- ml_ref$suggest_terms(
        terms = methods$metadata$search_terms,
        max_suggestions = max_suggestions,
        min_similarity = min_similarity,
        performance_data = extract_term_perf(methods$search_results, methods$metadata$search_terms)
      )

      return(suggestions)
    }

    methods$eval_terms <- function(suggested_terms, validation_corpus = NULL) {
      if (is.null(validation_corpus)) {
        validation_corpus <- methods$search_results
      }

      evaluation <- ml_ref$eval_suggestions(
        original_terms = methods$metadata$search_terms,
        suggested_terms = suggested_terms,
        validation_corpus = validation_corpus,
        gold_standard = methods$gold_standard
      )

      return(evaluation)
    }

    methods$has_ml_terms <- function() {
      return(TRUE)
    }

    methods$get_ml_engine <- function() {
      return(ml_ref)
    }

    return(methods)
  }

  # Execute wrapper and return methods
  result <- wrapper()
  class(result) <- c("SimpleMLWrapper", "list")
  return(result)
}

#' Automated Search Strategy Improvement Workflow
#'
#' @param search_results Current search results data frame
#' @param search_strategy Current search strategy
#' @param gold_standard Optional gold standard for evaluation
#' @param improvement_iterations Number of improvement iterations
#' @return Improved search strategy with performance comparison
#' @export
improve_strategy <- function(search_results, search_strategy,
                             gold_standard = NULL, improvement_iterations = 3) {

  cat("Starting automated search strategy improvement...\n")

  # Initialize components using factory approach to avoid R6 issues
  analyzer <- create_ml_analyzer(search_results, gold_standard, search_strategy)

  current_strategy <- search_strategy
  improvement_history <- list()

  for (iteration in 1:improvement_iterations) {
    cat("Improvement iteration", iteration, "of", improvement_iterations, "\n")

    # Calculate current performance
    current_metrics <- analyzer$calculate_metrics()

    # Generate term suggestions
    suggestions <- analyzer$suggest_terms(max_suggestions = 3, min_similarity = 0.7)

    # Create improved strategy variants
    strategy_variants <- list()

    # Single-term replacement variants
    for (term_name in names(suggestions)) {
      if (nrow(suggestions[[term_name]]$suggestions) > 0) {
        best_suggestion <- suggestions[[term_name]]$suggestions$term[1]

        variant_terms <- current_strategy$terms
        variant_terms[variant_terms == term_name] <- best_suggestion

        variant_strategy <- current_strategy
        variant_strategy$terms <- variant_terms
        variant_strategy$improvement_note <- paste("Replaced", term_name, "with", best_suggestion)

        strategy_variants[[paste0("replace_", term_name)]] <- variant_strategy
      }
    }

    # Multi-term replacement variant
    if (length(suggestions) > 0) {
      multi_terms <- current_strategy$terms
      for (term_name in names(suggestions)) {
        if (nrow(suggestions[[term_name]]$suggestions) > 0) {
          best_suggestion <- suggestions[[term_name]]$suggestions$term[1]
          multi_terms[multi_terms == term_name] <- best_suggestion
        }
      }

      if (!identical(multi_terms, current_strategy$terms)) {
        multi_strategy <- current_strategy
        multi_strategy$terms <- multi_terms
        multi_strategy$improvement_note <- "Multiple term replacements"
        strategy_variants[["multi_replace"]] <- multi_strategy
      }
    }

    # Evaluate variants
    if (length(strategy_variants) > 0) {
      variant_performance <- lapply(strategy_variants, function(variant) {
        # Simulate search with variant
        variant_results <- sim_strategy_search(variant, search_results)

        if (!is.null(gold_standard)) {
          metrics <- calc_precision_recall(variant_results, gold_standard)
          return(list(
            strategy = variant,
            metrics = metrics,
            f1_score = metrics$f1_score,
            result_count = length(variant_results)
          ))
        } else {
          return(list(
            strategy = variant,
            metrics = NULL,
            f1_score = length(variant_results) / max(nrow(search_results), 1),  # Use result ratio as proxy
            result_count = length(variant_results)
          ))
        }
      })

      # Select best variant
      f1_scores <- sapply(variant_performance, function(x) x$f1_score)
      best_variant_idx <- which.max(f1_scores)

      # Check if improvement was made
      baseline_f1 <- if (!is.null(gold_standard)) {
        baseline_results <- sim_strategy_search(current_strategy, search_results)
        calc_precision_recall(baseline_results, gold_standard)$f1_score
      } else {
        baseline_results <- sim_strategy_search(current_strategy, search_results)
        length(baseline_results) / max(nrow(search_results), 1)
      }

      if (f1_scores[best_variant_idx] > baseline_f1) {
        cat("Improvement found! F1 score increased from", round(baseline_f1, 3),
            "to", round(f1_scores[best_variant_idx], 3), "\n")

        # Update current strategy
        current_strategy <- variant_performance[[best_variant_idx]]$strategy

        # Record improvement
        improvement_history[[iteration]] <- list(
          iteration = iteration,
          baseline_f1 = baseline_f1,
          improved_f1 = f1_scores[best_variant_idx],
          improvement = f1_scores[best_variant_idx] - baseline_f1,
          strategy_used = variant_performance[[best_variant_idx]]$strategy,
          suggestions_applied = suggestions
        )
      } else {
        cat("No improvement found in iteration", iteration, "\n")
        improvement_history[[iteration]] <- list(
          iteration = iteration,
          baseline_f1 = baseline_f1,
          improved_f1 = baseline_f1,
          improvement = 0,
          strategy_used = current_strategy,
          suggestions_applied = suggestions
        )
      }
    } else {
      cat("No strategy variants generated in iteration", iteration, "\n")
    }
  }

  # Final evaluation
  final_results <- sim_strategy_search(current_strategy, search_results)
  final_metrics <- if (!is.null(gold_standard)) {
    calc_precision_recall(final_results, gold_standard)
  } else NULL

  result <- list(
    original_strategy = search_strategy,
    improved_strategy = current_strategy,
    performance_comparison = list(
      original = if (!is.null(gold_standard)) {
        orig_results <- sim_strategy_search(search_strategy, search_results)
        calc_precision_recall(orig_results, gold_standard)
      } else NULL,
      improved = final_metrics
    ),
    improvement_history = improvement_history,
    total_improvement = if (!is.null(final_metrics) && !is.null(gold_standard)) {
      orig_results <- sim_strategy_search(search_strategy, search_results)
      orig_metrics <- calc_precision_recall(orig_results, gold_standard)
      final_metrics$f1_score - orig_metrics$f1_score
    } else NULL,
    iterations_performed = improvement_iterations
  )

  class(result) <- "improved_strategy"
  return(result)
}

#' Extract Term Performance from Search Results
#'
#' @param search_results Data frame with search results
#' @param terms Character vector of terms to analyze
#' @return Data frame with term performance metrics
extract_term_perf <- function(search_results, terms) {
  performance_data <- data.frame(
    term = terms,
    performance = numeric(length(terms)),
    frequency = numeric(length(terms)),
    stringsAsFactors = FALSE
  )

  for (i in seq_along(terms)) {
    term <- terms[i]

    # Count occurrences in titles and abstracts
    title_matches <- sum(grepl(tolower(term), tolower(search_results$title), fixed = TRUE))
    abstract_matches <- if ("abstract" %in% names(search_results)) {
      sum(grepl(tolower(term), tolower(search_results$abstract), fixed = TRUE))
    } else 0

    total_matches <- title_matches + abstract_matches

    # Calculate performance score (could be refined with more sophisticated metrics)
    performance_score <- total_matches / nrow(search_results)

    performance_data$performance[i] <- performance_score
    performance_data$frequency[i] <- total_matches
  }

  return(performance_data)
}

#' Visualize Term Suggestion Results
#'
#' @param suggestion_results Results from MLTermEngine
#' @param evaluation_results Optional evaluation results
#' @return ggplot object
#' @export
plot_term_suggestions <- function(suggestion_results, evaluation_results = NULL) {
  # Prepare data for visualization
  plot_data <- do.call(rbind, lapply(names(suggestion_results), function(term_name) {
    suggestions <- suggestion_results[[term_name]]$suggestions
    if (nrow(suggestions) > 0) {
      data.frame(
        original_term = term_name,
        suggested_term = suggestions$term,
        predicted_performance = suggestions$predicted_performance,
        confidence = suggestions$confidence,
        rank = seq_len(nrow(suggestions)),
        stringsAsFactors = FALSE
      )
    } else {
      data.frame()
    }
  }))

  if (nrow(plot_data) == 0) {
    warning("No suggestion data to plot")
    return(NULL)
  }

  # Create performance comparison plot
  p1 <- ggplot2::ggplot(plot_data, ggplot2::aes(x = reorder(.data$suggested_term, .data$predicted_performance),
                                                y = .data$predicted_performance,
                                                fill = .data$original_term)) +
    ggplot2::geom_col(alpha = 0.8) +
    ggplot2::facet_wrap(~ .data$original_term, scales = "free_y") +
    ggplot2::coord_flip() +
    ggplot2::labs(
      title = "ML-Based Term Suggestions",
      subtitle = "Predicted performance scores for suggested synonymous terms",
      x = "Suggested Terms",
      y = "Predicted Performance Score",
      fill = "Original Term"
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(
      plot.title = ggplot2::element_text(size = 14, face = "bold"),
      plot.subtitle = ggplot2::element_text(size = 12, color = "gray60"),
      legend.position = "none"
    )

  # Add confidence intervals if available
  if ("confidence" %in% names(plot_data)) {
    p1 <- p1 +
      ggplot2::geom_errorbar(
        ggplot2::aes(ymin = .data$predicted_performance * (1 - .data$confidence * 0.1),
                     ymax = .data$predicted_performance * (1 + .data$confidence * 0.1)),
        width = 0.2, alpha = 0.6
      )
  }

  return(p1)
}

#' Generate Term Suggestion Report
#'
#' @param suggestion_results Results from term suggestion analysis
#' @param evaluation_results Optional evaluation results
#' @param output_path Output file path for report
#' @return Path to generated report
#' @export
gen_term_report <- function(suggestion_results, evaluation_results = NULL,
                            output_path = NULL) {

  if (is.null(output_path)) {
    output_path <- file.path(tempdir(), "term_suggestion_report.html")
  }

  # Create report content
  report_lines <- c(
    "# Machine Learning Term Suggestion Report",
    "",
    paste("Generated:", Sys.time()),
    "",
    "## Summary",
    "",
    paste("- **Original terms analyzed:**", length(suggestion_results)),
    paste("- **Total suggestions generated:**", sum(sapply(suggestion_results, function(x) nrow(x$suggestions)))),
    "",
    "## Term-by-Term Analysis",
    ""
  )

  # Add analysis for each term
  for (term_name in names(suggestion_results)) {
    term_data <- suggestion_results[[term_name]]

    report_lines <- c(report_lines,
                      paste("### Original Term:", term_name),
                      "",
                      paste("**Prediction Confidence:**", round(term_data$prediction_confidence, 3)),
                      ""
    )

    if (nrow(term_data$suggestions) > 0) {
      report_lines <- c(report_lines, "**Top Suggestions:**", "")

      for (i in 1:min(3, nrow(term_data$suggestions))) {
        suggestion <- term_data$suggestions[i, ]
        report_lines <- c(report_lines,
                          paste(i, ".", suggestion$term,
                                "- Score:", round(suggestion$predicted_performance, 3),
                                "- Confidence:", round(suggestion$confidence, 3))
        )
      }
      report_lines <- c(report_lines, "")
    } else {
      report_lines <- c(report_lines, "No suggestions generated for this term.", "")
    }
  }

  # Add evaluation results if available
  if (!is.null(evaluation_results)) {
    report_lines <- c(report_lines,
                      "## Performance Evaluation",
                      "",
                      paste("**Original retrieval count:**", evaluation_results$retrieval_comparison$original_count),
                      paste("**Suggested retrieval count:**", evaluation_results$retrieval_comparison$suggested_count),
                      paste("**Overlap:**", evaluation_results$retrieval_comparison$overlap),
                      paste("**Unique to suggestions:**", evaluation_results$retrieval_comparison$unique_to_suggested),
                      ""
    )

    if (!is.null(evaluation_results$performance_comparison)) {
      perf_comp <- evaluation_results$performance_comparison
      report_lines <- c(report_lines,
                        "### Performance Metrics Comparison",
                        "",
                        paste("**Precision improvement:**",
                              sprintf("%+.3f", perf_comp$improvement$precision_delta)),
                        paste("**Recall improvement:**",
                              sprintf("%+.3f", perf_comp$improvement$recall_delta)),
                        paste("**F1 score improvement:**",
                              sprintf("%+.3f", perf_comp$improvement$f1_delta)),
                        ""
      )
    }

    if (!is.null(evaluation_results$semantic_analysis)) {
      sem_analysis <- evaluation_results$semantic_analysis
      report_lines <- c(report_lines,
                        "### Semantic Diversity Analysis",
                        "",
                        paste("**Average similarity:**", round(sem_analysis$average_similarity, 3)),
                        paste("**Diversity score:**", round(sem_analysis$diversity_score, 3)),
                        paste("**Unique concepts:**", sem_analysis$unique_concepts),
                        ""
      )
    }
  }

  # Write report
  if (grepl("\\.html$", output_path)) {
    # Convert markdown to HTML for better formatting
    html_content <- c(
      "<!DOCTYPE html>",
      "<html><head><title>Term Suggestion Report</title>",
      "<style>body{font-family:Arial,sans-serif;margin:40px;} h1,h2,h3{color:#333;} code{background:#f4f4f4;padding:2px;}</style>",
      "</head><body>",
      md_to_html(report_lines),
      "</body></html>"
    )
    writeLines(html_content, output_path)
  } else {
    writeLines(report_lines, output_path)
  }

  return(output_path)
}

#' Simple Markdown to HTML Converter
#'
#' @param markdown_lines Character vector of markdown lines
#' @return Character vector of HTML lines
md_to_html <- function(markdown_lines) {
  html_lines <- character(length(markdown_lines))

  for (i in seq_along(markdown_lines)) {
    line <- markdown_lines[i]

    if (grepl("^# ", line)) {
      html_lines[i] <- paste0("<h1>", gsub("^# ", "", line), "</h1>")
    } else if (grepl("^## ", line)) {
      html_lines[i] <- paste0("<h2>", gsub("^## ", "", line), "</h2>")
    } else if (grepl("^### ", line)) {
      html_lines[i] <- paste0("<h3>", gsub("^### ", "", line), "</h3>")
    } else if (grepl("^\\*\\*.*\\*\\*", line)) {
      html_lines[i] <- gsub("\\*\\*(.*?)\\*\\*", "<strong>\\1</strong>", line)
    } else if (line == "") {
      html_lines[i] <- "<br>"
    } else {
      html_lines[i] <- paste0("<p>", line, "</p>")
    }
  }

  return(html_lines)
}
